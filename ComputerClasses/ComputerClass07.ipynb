{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSE ST451: Bayesian Machine Learning\n",
    "## Author: Kostas Kalogeropoulos\n",
    "\n",
    "## Week 7: Mixture Models\n",
    "\n",
    "Topics covered \n",
    " - Fitting Gaussian Mixture models using the EM algorithm\n",
    " - Obtaining information on soft allocation of individuals\n",
    " - Model Choice within the family of Gaussian Mixtures\n",
    " - Bayesian approach with overfitted mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standaer libraries will be used with the addition of two new ones from sklearn for the EM and Variational Bayes approach on Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "#The next two lines import the functions for the two things we will look into today\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Iris dataset\n",
    "\n",
    "The Iris Dataset. This data sets consists of 3 different types of irises' (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray. The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width. The below plot uses the first two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "#next we import it into a pandas frame for convenience (not necessry)\n",
    "pdiris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "print(pdiris.shape)\n",
    "pdiris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots \n",
    "\n",
    "Below we will see some 2d plots just to get a feel of the data. There appears to be some clustering but it is hard to infer the number of clusters of the 4d datasets from 2d plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pdiris['sepal length (cm)'], pdiris['petal length (cm)'], 'o')\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('petal length (cm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pdiris['sepal width (cm)'], pdiris['petal width (cm)'], 'o')\n",
    "plt.xlabel('sepal width (cm)')\n",
    "plt.ylabel('petal width (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting GMMs using the EM algorithm\n",
    "\n",
    "The code for doing so is given below. The '.fit' bit obtains the MLEs of means and covariances that can be viewed using '.means_' and '.covariances_'\n",
    "\n",
    "We start by inspecting and visualisibg a 2-d dataset with only the 'sepal width (cm)' and 'petal width (cm)' variables. The full dataset is analysed afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['sepal width (cm)','petal width (cm)']\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(pdiris[vars])\n",
    "print(gmm.means_)\n",
    "print('\\n')\n",
    "print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft allocation of individuals to clusters\n",
    "\n",
    "GMM method does not necesarily allocates individuals with certainty but with probabilities.\n",
    "\n",
    "Adding the probabilities can give as an ideas of how many people each cluster has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = gmm.predict_proba(pdiris[vars])\n",
    "print(np.sum(probs,axis=0))\n",
    "print(probs[21:50].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Search\n",
    "\n",
    "We need to fit models wiht different numbers of cluster and different type of covariance matrices to identify the best one. This is done via the BIC (the smaller the better in this case)\n",
    "\n",
    "Types of covariance matrices:\n",
    " - spherical: each cluster k has covariance $\\sigma^2_k I$\n",
    " - tied: full covariance matrix but the same across clusters\n",
    " - diag: diagonal covariance matrix, different for each cluster\n",
    " - full: full covariance matrix, different for each cluster\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_bic = np.infty\n",
    "\n",
    "#Consider k=1,...,6 and four types of covariance matrix\n",
    "n_components_range = range(1, 9)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "bic = np.zeros((len(n_components_range),len(cv_types))) #matrix to store the BICs\n",
    "j = -1\n",
    "for cv_type in cv_types:\n",
    "    j = j+1\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = GaussianMixture(n_components=n_components,\n",
    "                                      covariance_type=cv_type)\n",
    "        gmm.fit(pdiris)\n",
    "        bicij = gmm.bic(pdiris)  #get the BIC \n",
    "        bic[n_components-1,j] = bicij\n",
    "        #the code below keeps track of the model with the lowest BIC\n",
    "        if bicij < lowest_bic:\n",
    "            lowest_bic = bicij\n",
    "            best_gmm = gmm\n",
    "print(lowest_bic)\n",
    "bic = pd.DataFrame(bic,columns = cv_types,index=n_components_range)\n",
    "bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_gmm.means_)\n",
    "print('\\n')\n",
    "print(best_gmm.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1\n",
    "\n",
    "Repeat the analysis using only two of the four variables. Do we get a different conclusion on the number of clusters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate data to test the method\n",
    "\n",
    "So far we have been looking at a dataset where we are not sure about the 'true' number of clusters and type of covariance matrix.\n",
    "\n",
    "In what follows we will simulate data from a Gaussian mixture with three components and spherical covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per component\n",
    "n_samples = 200\n",
    "np.random.seed(5)\n",
    "\n",
    "# Generate random sample, three components 1.mean (0,0) cov=1I, 2. mean (-6, 3) cov=.49I and\n",
    "# 3. mean (3, -4) cov=4I\n",
    "C = np.array([[0., -0.1], [1.7, .4]])\n",
    "X = np.r_[np.random.randn(n_samples, 2), \n",
    "          .4 * np.random.randn(n_samples, 2) + np.array([-6, 3]), \n",
    "         3* np.random.randn(n_samples, 2) + np.array([3, -4])] \n",
    "print(X.shape)\n",
    "gmm = GaussianMixture(n_components=3,covariance_type='spherical')\n",
    "gmm.fit(X)\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)\n",
    "labels = gmm.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels)                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the method\n",
    "\n",
    "Below we repeat the previous model search procedure to the data contained in X. We would like to test whethere the optimal models will indeed be the one with three components and spherical covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_bic = np.infty\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "bic = np.zeros((len(n_components_range),len(cv_types)))\n",
    "j = -1\n",
    "for cv_type in cv_types:\n",
    "    j = j+1\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = GaussianMixture(n_components=n_components,\n",
    "                                      covariance_type=cv_type)\n",
    "        gmm.fit(X)\n",
    "        bicij = gmm.bic(X)\n",
    "        bic[n_components-1,j] = bicij\n",
    "        if bicij < lowest_bic:\n",
    "            lowest_bic = bicij\n",
    "            best_gmm = gmm\n",
    "print(lowest_bic)\n",
    "bic = pd.DataFrame(bic,columns = cv_types,index=n_components_range)\n",
    "bic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "Conduct another simulation experiment generating data from a Gaussian mixture. Choose your own number of components, means and covariances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitted Mixtures\n",
    "\n",
    "Now we will explore what happens when we fit a model with more components than the ones in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=6,covariance_type='spherical')\n",
    "gmm.fit(X)\n",
    "probs = gmm.predict_proba(X)\n",
    "results = np.sum(probs,axis=0)\n",
    "results = pd.DataFrame(results.round(0), columns = ['# of individuals'], index=range(1,7))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = gmm.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Gaussian Mixture models\n",
    "\n",
    "We will also apply the fully Bayesian model to the same data with a Dirichlet prior on the cluster probabilities with a low hyperparameter (weight_concentration_prior) of 0.01. This choice is going to penalise redundant clusters by not allocating individuals to them unless it is necessaray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bgmm = BayesianGaussianMixture(n_components=6,covariance_type='full',\n",
    "                               weight_concentration_prior=0.01, max_iter = 200)\n",
    "Bgmm.fit(X)\n",
    "probs = Bgmm.predict_proba(X)\n",
    "results = np.sum(probs,axis=0)\n",
    "results = pd.DataFrame(results.round(0), columns = ['# of individuals'], index=range(1,7))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Bgmm.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the fully Bayesian approach to the Iris dataset and check the resulting number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
