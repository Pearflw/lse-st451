{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSE ST451: Bayesian Machine Learning\n",
    "## Author: Kostas Kalogeropoulos\n",
    "\n",
    "## Week 8: Markov Chain Monte Carlo / Bayesian Sparse Linear Regression\n",
    "\n",
    "Topics covered \n",
    " - Sampling from the posterior using the Gibbs Sampler in Python\n",
    " - Presenting the output of a Markov Chain Monte Carlo (MCMC) ouput \n",
    " - Using Hamiltonian MCMC to sample from the posterior using Stan\n",
    " - Presenting MCMC output in Stan\n",
    " - Bayesian Sparse Linear Regression with the *horseshoe* prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use standard Python libraries (that have been used before). For Stan we will continue in RStudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs sampler\n",
    "\n",
    "We will first code a Gibbs Sampler for the model where $y=(y_1,\\dots,y_n)$ are independent $N(\\mu,\n",
    "\\sigma^2)$ random variables with both $\\mu$ and $\\sigma^2$ unknown.\n",
    "\n",
    "We will assign the $N(0,\\omega^2)$ prior on $\\mu$ and the $IGamma(\\alpha,\\beta)$ prior on $\\sigma^2$.\n",
    "\n",
    "Then we get that \n",
    "\n",
    "$$\n",
    "\\mu |y,\\sigma^2 \\sim N\\left(\\frac{\\omega^2\\bar{y}}{\\omega^2 + \\sigma^2/n},\\frac{\\omega^2 \\sigma^2/n}{\\omega^2 + \\sigma^2/n}\\right)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\sigma^2|y,\\mu \\sim IGamma (n/2+\\alpha, \\beta+\\frac{\\sum_{i=1}^{n}(y_i-\\bar{y})^2+n(\\bar{y}-\\mu)^2}{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate data to check the Gibbs sampler\n",
    "n = 100 #sample size\n",
    "sigma_true = 2\n",
    "sigma2_true = sigma_true**2\n",
    "mu_true = 5\n",
    "np.random.seed(4)\n",
    "y = mu_true + np.random.randn(n)*sigma_true\n",
    "ybar = np.mean(y)\n",
    "S2 = np.sum((y-ybar)**2)\n",
    "print('data:',y)\n",
    "print('sample mean:',ybar)\n",
    "print('sample variance:',S2/(n-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we run the Gibbs sampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000 # number of iterations\n",
    "\n",
    "#create object mus and sigma2s to store the Markov chain samples, \n",
    "#with $\\pi(\\theta,\\sigma^2|y)$ as stationary distribution\n",
    "out_mu = np.zeros(N)\n",
    "out_sigma2 = np.zeros(N)\n",
    "\n",
    "#Set prior parameters\n",
    "omega2 = 1000\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "\n",
    "#Set initial values for mu and sigma2\n",
    "mu = -0\n",
    "sigma2 = 10\n",
    "\n",
    "for i in range(N):\n",
    "    #Store current value of the chain (mu,sigma2)\n",
    "    out_mu[i] = mu\n",
    "    out_sigma2[i] = sigma2\n",
    "    \n",
    "    #Draw mu given y and sigma2\n",
    "    denominator = omega2 + (sigma2/n)\n",
    "    M = omega2*ybar/denominator          #Mean of the full conditional of mu\n",
    "    V = omega2*(sigma2/n)/denominator    #Variance of the full conditional of mu \n",
    "    mu = M + np.sqrt(V)*np.random.randn(1)  #Draw from the full conditional of mu\n",
    "    \n",
    "    #Draw sigma2 given y and mu\n",
    "    A = 0.5*n + alpha                  #A parameter of the full conditional of sigma2\n",
    "    B = beta + 0.5*(S2+ n*((ybar-mu)**2)) #B parameter of the full conditional of sigma2\n",
    "    sigma2 = 1/np.random.gamma(A, 1/B, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(out_mu,label='posterior traceplot of mu')\n",
    "plt.plot(np.ones(N)*mu_true,label='\"true\" value of mu')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(out_sigma2,label='posterior traceplot of sigma2')\n",
    "plt.plot(np.ones(N)*sigma2_true,label='\"true\" value of sigma2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#present the output via a pandas data frame\n",
    "mmu = np.mean(out_mu)\n",
    "medmu = np.median(out_mu)\n",
    "pctmu = np.percentile(out_mu,(2.5,97.5))\n",
    "msig = np.mean(out_sigma2)\n",
    "medsig = np.median(out_sigma2)\n",
    "pctsig = np.percentile(out_sigma2,(2.5,97.5))\n",
    "results = np.array([[mmu,medmu,pctmu[0],pctmu[1]],[msig,medsig,pctsig[0],pctsig[1]]]).round(3)\n",
    "col = ['Posterior mean','Posterior median','Posterior 2.5% point','Posterior 97.5% point']\n",
    "ind = ['mu','sigma2']\n",
    "results = pd.DataFrame(results,columns = col,index=ind)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1\n",
    "\n",
    "Repeat for the model below\n",
    "\n",
    "$$\n",
    "y_i\\sim \\text{Poisson}(\\mu), \\;\\;i=1,...,N,\n",
    "$$\n",
    "with priors \n",
    "$$\n",
    "\\mu \\sim \\text{Gamma}(2,\\beta),\\;\\;\n",
    "\\beta \\sim \\text{Exponential}(1)\n",
    "$$\n",
    "\n",
    "It can be shown (good exercise) that the full conditionals for $\\mu,\\beta$ are \n",
    "\n",
    "$$\n",
    "\\mu| y,\\beta \\;\\sim\\;\\text{Gamma}(2 + \\sum_i y_i,n+\\beta)\n",
    "$$\n",
    "$$\n",
    "\\beta|y,\\mu\\;\\sim \\;\\text{Gamma}(3,1+\\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Sparse Linear Regression\n",
    "\n",
    "We will illustrate several Bayesian (or not) approaches to handle sparsity in linear regression on the diabetes dataset.\n",
    "\n",
    "Some info on the data is given below:\n",
    "\n",
    " - 442 diabetes patients\n",
    " - 10 main variables, x_1,...,x_{10} : age, gender, body mass index, average blood pressure (map), and six blood serum measurements (tc, ldl, hdl, tch, ltg, glu)\n",
    " - 45 interactions of the form xjxk\n",
    " - 9 quadratic effects of the form x_j^2 (gender is binary, so x^2=x_2^2)\n",
    " - measure of disease progression taken one year later, $y$\n",
    "\n",
    "We have a total of $p=64$ variables that we might use to predict $y$. It seems plausible that a subset of the variables will reliably predict $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the `diabetes.data.txt` file to get y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetesY = pd.read_csv('diabetes.data.txt',sep=\"\\t\")\n",
    "diabetesY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetesY['Y']\n",
    "y = (y - np.mean(y))/np.std(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the `data64.txt` file to get X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetesX = pd.read_csv('data64.txt', sep=\"\\t\")\n",
    "X = diabetesX.values\n",
    "X = (X - np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100/442, random_state=5)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start fitting models. We start with the **null model** (no covariates) and the **OLS** (all covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null model use the sample mean of y\n",
    "print('null: ',np.sum((y_test - np.mean(y_train))**2) / len(y_test))\n",
    "\n",
    "# OLS \n",
    "linreg = LinearRegression(fit_intercept=False,normalize=False)\n",
    "linreg.fit(X_train,y_train)\n",
    "y_pred = linreg.predict(X_test)\n",
    "print('OLS: ',np.sum((y_test - y_pred)**2) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply **Ridge** and **Lasso** regression. For each of these, we search for the penalty parameter that minimises the MSE and report that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "Lambdas = np.linspace(0.5,50,100)\n",
    "MSEs = np.zeros(100)\n",
    "for i in range(100):\n",
    "    ridgereg = Ridge(fit_intercept=False,normalize=False,alpha=Lambdas[i])\n",
    "    ridgereg.fit(X_train,y_train)\n",
    "    y_pred = ridgereg.predict(X_test)\n",
    "    MSEs[i] = np.sum((y_test - y_pred)**2) / len(y_test)\n",
    "print('Ridge: ',np.min(MSEs),' lambda: ',Lambdas[np.argmin(MSEs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "Lambdas = np.linspace(1e-2,1,100)\n",
    "MSEs = np.zeros(100)\n",
    "for i in range(100):\n",
    "    lassoreg = Lasso(fit_intercept=False,normalize=False,alpha=Lambdas[i])\n",
    "    lassoreg.fit(X_train,y_train)\n",
    "    y_pred = lassoreg.predict(X_test)\n",
    "    MSEs[i] = np.sum((y_test - y_pred)**2) / len(y_test)\n",
    "print('Lasso: ',np.min(MSEs),' lambda: ',Lambdas[np.argmin(MSEs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save all the data to load them in RStudio and work with Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_train.csv', X_train, delimiter=',')\n",
    "np.savetxt('X_test.csv', X_test, delimiter=',')\n",
    "np.savetxt('y_train.csv', y_train, delimiter=',')\n",
    "np.savetxt('y_test.csv', y_test, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
