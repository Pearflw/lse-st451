{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSE ST451: Bayesian Machine Learning\n",
    "## Author: Kostas Kalogeropoulos\n",
    "\n",
    "## Week 6: Graphical Models\n",
    "\n",
    "Topics covered \n",
    " - Image processing\n",
    " - Ising model\n",
    " - Text classification\n",
    " - Document-term matrix\n",
    " - Naive Bayes Classifier\n",
    " - Working with Pipelines in Python\n",
    " - Adding progress bars in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the frequently used libraries we will also need the **Image** function from PIL as well as several functions of **sklearn** for text processing, Naive Bayes classifier and pipelines. Finally the progress bar is given by the **tqdm** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Image processing material\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#Text Classification Material\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Data\n",
    "\n",
    "Load image data from the file bayes.bmp which should be saved in the same directory with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image data\n",
    "data = Image.open('bayes.bmp')\n",
    "img = np.double(data)\n",
    "img_mean = np.mean(img)\n",
    "clean = +1*(img>img_mean) + -1*(img<img_mean)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(clean,cmap='Greys')\n",
    "plt.title(\"clean binary image\")\n",
    "[M,N]=clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add noise to the image to disort it\n",
    "\n",
    "We distort the image by adding normal error with 0 mean and standard deviation of 0.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma  = 0.6  #noise level\n",
    "y = clean + sigma*np.random.randn(M,N) #y_i ~ N(x_i; sigma^2);\n",
    "plt.figure()\n",
    "plt.imshow(y, cmap='Greys')\n",
    "plt.title(\"observed noisy image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Variational Bayes hyper-parameters\n",
    "\n",
    "We assume equal weights W_ij=1, a \\lambda of 0.5, while running the algorithm for 15 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 1  #coupling strength (w_ij)\n",
    "rate = 0.5  #update smoothing rate lambda\n",
    "max_iter = 15\n",
    "ELBO = np.zeros(max_iter)\n",
    "Hx_mean = np.zeros(max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean-Field VI\n",
    "print('running mean-field variational inference')\n",
    "logodds = multivariate_normal.logpdf(y.flatten(), mean=+1, cov=sigma**2) - \\\n",
    "          multivariate_normal.logpdf(y.flatten(), mean=-1, cov=sigma**2)\n",
    "# y.flatten converts the y matrix into a vector\n",
    "logodds = np.reshape(logodds, (M, N))\n",
    "\n",
    "#init\n",
    "p1 = sigmoid(logodds)\n",
    "mu = 2*p1-1  #mu_init\n",
    "\n",
    "a = mu + 0.5 * logodds\n",
    "qxp1 = sigmoid(+2*a)  #q_i(x_i=+1)\n",
    "qxm1 = sigmoid(-2*a)  #q_i(x_i=-1)\n",
    "\n",
    "logp1 = np.reshape(multivariate_normal.logpdf(y.flatten(), mean=+1, cov=sigma**2), (M, N))\n",
    "logm1 = np.reshape(multivariate_normal.logpdf(y.flatten(), mean=-1, cov=sigma**2), (M, N))\n",
    "\n",
    "for i in tqdm(range(max_iter)):\n",
    "    muNew = mu\n",
    "    for ix in range(N):\n",
    "        for iy in range(M):\n",
    "            pos = iy + M*ix\n",
    "            #The following code sets up the neighbourhood around the index\n",
    "            neighborhood = pos + np.array([-1,1,-M,M])            \n",
    "            boundary_idx = [iy!=0,iy!=M-1,ix!=0,ix!=N-1]\n",
    "            neighborhood = neighborhood[np.where(boundary_idx)[0]]            \n",
    "            xx, yy = np.unravel_index(pos, (M,N), order='F')\n",
    "            nx, ny = np.unravel_index(neighborhood, (M,N), order='F')\n",
    "            \n",
    "            Sbar = J*np.sum(mu[nx,ny])       \n",
    "            muNew[xx,yy] = (1-rate)*muNew[xx,yy] + rate*np.tanh(Sbar + 0.5*logodds[xx,yy])\n",
    "            ELBO[i] = ELBO[i] + 0.5*(Sbar * muNew[xx,yy])\n",
    "    mu = muNew\n",
    "            \n",
    "    a = mu + 0.5 * logodds\n",
    "    qxp1 = sigmoid(+2*a) #q_i(x_i=+1)\n",
    "    qxm1 = sigmoid(-2*a) #q_i(x_i=-1)    \n",
    "    #Hx = -qxm1*np.log(qxm1+1e-10) - qxp1*np.log(qxp1+1e-10) #entropy        \n",
    "    \n",
    "    ELBO[i] = ELBO[i] + np.sum(qxp1*logp1 + qxm1*logm1) + np.sum(Hx)\n",
    "    #Hx_mean[i] = np.mean(Hx)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ELBO to check convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ELBO, color='b', lw=2.0, label='ELBO')\n",
    "plt.title('Variational Inference for Ising Model')\n",
    "plt.xlabel('iterations'); plt.ylabel('ELBO objective')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally check if the image was restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mean = np.mean(mu)\n",
    "x = +1*(mu>img_mean) + -1*(mu<img_mean)\n",
    "plt.figure()\n",
    "plt.imshow(x,cmap='Greys')\n",
    "plt.title(\"after %d mean-field iterations\" %max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1\n",
    "\n",
    "Repeat the previous procedure changing some the specifications. For example you can try \n",
    " - adding more noise\n",
    " - changing the smooting rate \n",
    " - having different sigma than the one used to distort the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification \n",
    "\n",
    "Import the 20 newsgroups dataset, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It was originally collected by Ken Lang. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_train.target_names #prints all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text into a Document-Term matrix using the Count Vectorizer\n",
    "\n",
    "The following code will create a huge sparse Document - term matrix, the rows of which represent rows and the columns represent the number of times the words appear in a particular document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "Xnames = count_vect.get_feature_names()\n",
    "X_train_counts.shape\n",
    "print(Xnames[99000:99040])\n",
    "print(X_train_counts.toarray()[0,9000:9100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF:  \n",
    "Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency (TF - Term Frequencies) i.e. #count(word) / #Total words, in each document.\n",
    "\n",
    "### TF-IDF: \n",
    "Finally, we can even reduce the weightage of more common words like (the, is, an etc.) which occurs in all document. This is called as TF-IDF i.e Term Frequency times inverse document frequency.\n",
    "\n",
    "We can achieve both using below line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the multinomial Naive Bayes classifier\n",
    "\n",
    "Use .fit in order to fit the multinomial Naive Bayes classifier to a set of data (X matrix) and a categorical target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pipeline\n",
    "\n",
    "We can put all the processing steps together so that we don't have to re write them when considering other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Predictive Performance of the Naive Bayes Classifier\n",
    "\n",
    "Check against a test (unseen) subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data) \n",
    "print('accuracy rate: ')\n",
    "print(np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving performance\n",
    "\n",
    "We can improve the performance by removing stop words\n",
    "ii. reducing the Laplace smoothing, setting alpha to 0.1 rather than the default which is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB(alpha=0.1))])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data) \n",
    "print('accuracy rate: ')\n",
    "print(np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "Experiment by trying different options on the user-specified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
