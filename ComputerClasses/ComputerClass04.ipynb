{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSE ST451: Bayesian Machine Learning\n",
    "## Author: Kostas Kalogeropoulos\n",
    "\n",
    "## Week 4: Classification\n",
    "\n",
    "Topics covered \n",
    " - Working with 'while' loops in Python\n",
    " - Coding Newton-Rapshon optimisation in Python\n",
    " - Finding Maximum Likelihood Estimates of logistic regression coefficients\n",
    " - Fitting Bayesian logistic Regression models and summarising the posterior of their coefficients\n",
    " - Calculate the model evidence, BIC for model choice under Bayesian logistic regression \n",
    " - Evaluate predictive performance for binary data: missclassification rate, sensitivity, specificity, ROC curves and area under them, log scoring rule\n",
    " - Fitting linear discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with loading the necessary libraries. We will pretty much the same libraries as next week with three additions: **scipy.optimize** for optimisation, **sklearn.discriminant_analysis** and some merics for prediction assessment from **sklearn.metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import scipy as sc\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt #for plots\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, roc_auc_score \n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression via Maximum Likelihood\n",
    "\n",
    "Model directly the probability $p(y=1|x)$ for binary data $y \\in \\{0,1\\}$ for some covariates $X$. There is no closed form for the MLE but we can solve approximately with an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breast Cancer Survival Data\n",
    "\n",
    "The dataset contains cases from a study that was conducted between\n",
    "   1958 and 1970 at the University of Chicago's Billings Hospital on\n",
    "   the survival of patients who had undergone surgery for breast\n",
    "   cancer.\n",
    "\n",
    "\n",
    "Four columns \n",
    "\n",
    "1. Age of patient at time of operation (numerical)\n",
    "2. Patient's year of operation (year - 1900, numerical)\n",
    "3. Number of positive axillary nodes detected (numerical)\n",
    "4. Survival status (class attribute)\n",
    "     1 = the patient survived 5 years or longer\n",
    "     2 = the patient died within 5 year\n",
    "\n",
    "See more info at the UCI repository [page](https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.names). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_hospital = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\"\n",
    "data1 = pd.read_csv(url_hospital, header=None,\n",
    "        names=['age', 'year', 'nodes_detected', 'survival_status'])\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data and create train test splits\n",
    "data1['y'] = data1.survival_status.replace(2, 0).values\n",
    "n = len(data1['y'])\n",
    "data1['x0'] = np.ones(n)\n",
    "predictors = ['x0','age','nodes_detected']\n",
    "target = ['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1[predictors], data1[target],\n",
    "                        test_size=0.33, random_state=1)\n",
    "n,p = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE using an optimizer\n",
    "\n",
    "The accompanying chapter in **Bishop is 4.3.2**.\n",
    "\n",
    "In our setting we have a binary (2-class) classification problem with a sample size $n$. Our date consistts of pairs of labels and features $\\{(y_1,x_1), \\ldots, (y_n,x_n)\\}$ where $y_i \\in \\{0,1\\}$ and $x_i \\in R^m$. We are interested in the probability $p(y=1|x)$.\n",
    "\n",
    "The logistic regression model uses the logistic function $\\sigma(\\cdot) : (-\\infty, -\\infty) \\rightarrow  (0,1) $ (also known as sigmoid)\n",
    "$$\\sigma(x) =\\big( \\frac{1}{1+e^{-x}} \\big)$$\n",
    "\n",
    "\n",
    "Note that the inverse of the the logistic function $\\sigma(\\cdot)^{-1}$, in the sense of their composition equaling the identity map, is the logit function $$\\text{logit}(a) = \\log\\big( \\frac{a}{1-a} \\big)$$\n",
    "\n",
    "Specifically, in the context of logistic regression, the function we need is the following\n",
    "\n",
    "$$\n",
    "\\sigma(x, \\beta) = \\big( \\frac{1}{1+e^{-x \\beta}} \\big)\n",
    "$$\n",
    "\n",
    "which we implement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigma(x,beta):\n",
    "    xbeta = x.dot(beta)\n",
    "    d = 1. + np.exp(-xbeta)\n",
    "    return d**(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression likelihood is given by the following formula\n",
    "\n",
    "\\begin{align}\n",
    "p( y=1| \\beta,x) &= \\prod_{i=1}^n f(x_n,\\beta)^{y_n} \\cdot [1-f(x_n,\\beta)]^{(1-y_n)} \\\\\n",
    "&= \\prod_{i=1}^n f_n^{y_n} \\cdot (1-f_n)^{(1-y_n)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "where we define for convenience of notation $f_n := f(x_n,\\beta)$ .\n",
    "\n",
    "Hence the negative log-likelihood (NLL) becomes\n",
    "\\begin{align}\n",
    "\\log p( y=1| \\beta,x) &= \\sum_{i=1}^n \\{ y_n \\log f_n + (1-y_n) \\log(1-f_n) \\}\n",
    "\\end{align}\n",
    "which we code as a function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(beta, x, y):\n",
    "    t = y.T.dot(np.log(sigma(x,beta)))+ (1-y).T.dot(np.log(1.-sigma(x,beta)))\n",
    "    return -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will find the MLE by minimising the negagive log-likelihood using the **Newton-Raphson** algorithm. This algorithm required the gradient ($f^{\\prime}$) and the Hessian matrix. Below we provide these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find Hessian\n",
    "def fprime(beta,x,y):\n",
    "    return  x.T.dot(sigma(x,beta)-y)\n",
    "def Hessian(beta,x):  \n",
    "    S = np.diag(sigma(x,beta)*(1-sigma(x,beta)))\n",
    "    hes = x.T.dot(S.dot(x))\n",
    "    return hes\n",
    "def NewtonRaphsonLogistReg(beta0,x,y,maxiter,tolerance):\n",
    "    i = 0\n",
    "    beta = beta0\n",
    "    NegLogLike = nll(beta,x,y)\n",
    "    AbDiff = 1; #anything bigger than tolerance would do here\n",
    "    while (AbDiff> tolerance) & (i<maxiter):\n",
    "        i =i+1;\n",
    "        print('iteration ',i,' Negative Log likelihood ',NegLogLike, ' AbDiff ', AbDiff)\n",
    "        grad = fprime(beta,x,y)\n",
    "        H = Hessian(beta,x)\n",
    "        H_inv = sc.linalg.inv(H) \n",
    "        beta = beta - H_inv.dot(grad)\n",
    "        NegLogLike_new = nll(beta,x,y)\n",
    "        AbDiff = np.abs(NegLogLike_new-NegLogLike)\n",
    "        NegLogLike = NegLogLike_new\n",
    "    if (iter == maxiter):\n",
    "        print('Did not Converge') \n",
    "    return beta, H_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply these functions to our data to obtain the MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = X_train.shape\n",
    "beta0 = np.zeros(p)\n",
    "tolerance = 1e-05\n",
    "maxiter = 100 \n",
    "beta_mle, cov = NewtonRaphsonLogistReg(beta0,X_train, y_train['y'],maxiter,tolerance)\n",
    "print(beta_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also compute 95% confidence intervals by using the Hessian which is the inverse of the MLE covariance. We will present the results via a pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert for the covariance and find 95% CIs\n",
    "#cov = sc.linalg.inv(H)\n",
    "se = np.sqrt(np.diag(cov))\n",
    "lower95 = beta_mle - 1.96*se\n",
    "upper95 = beta_mle + 1.96*se\n",
    "\n",
    "#present the output via a pandas data frame\n",
    "results = np.column_stack([beta_mle,se,lower95,upper95])\n",
    "col = ['coefficient','se','lower 95% bound','upper 95% bound']\n",
    "ind = ['intercept','age','nodes_detected']\n",
    "results = pd.DataFrame(results,columns = col,index=ind)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm we got it right we will also check with the function Logit from the statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age','nodes_detected']\n",
    "Xt = X_train[predictors]\n",
    "Xt = sm.add_constant(Xt)\n",
    "yt = y_train['y']\n",
    "model = sm.Logit(yt, Xt).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function LogisticRegression from sklearn can also be used. It is more robust and efficient for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['age','nodes_detected']\n",
    "Xt = X_train[predictors]\n",
    "logreg = LogisticRegression(fit_intercept=True,solver='lbfgs')\n",
    "logreg.fit(Xt,yt)\n",
    "intercept = np.array([logreg.intercept_])[0,]\n",
    "coef = np.array(logreg.coef_)[0,].T\n",
    "beta_mle = np.hstack((intercept,coef))\n",
    "print(beta_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Approximation (Bishop 4.5)\n",
    "\n",
    "Bayesian Logistic Regression is not availabe in a closed form. One method is to use a Laplace approximation according to which we approximate the posterior $p(\\beta | x, y)$ with a Gaussian. The Gaussian distribution is characterised by two parameters, the mean and the covariance. The mean $\\beta_{\\text{MAP}}$ is given by the MAP of the unormalized likelihood times prior, while the covariance is the Hessian evaluated $\\beta_{\\text{MAP}}$. \n",
    "\n",
    "The posterior is given by equation 4.141 and 4.142 which we implement below\n",
    "\n",
    "Assume a Gaussian prior and get the following approximate posterior \n",
    "\n",
    "\\begin{align}\n",
    "\\beta & \\sim N(m_0, S_0^{-1}) \\; \\text{(prior)}\\\\\n",
    "p(\\beta|x,y) & \\sim  N(\\hat{\\beta}, S^{-1})  \\; \\text{(posterior)}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\\begin{align}\n",
    "\\beta_{\\text{MAP}} & = \\text{argmin}_{\\beta} \\log p(\\beta|y,x), \\; \\log p(\\beta|y,x) = \\log p(y|\\beta,x) + \\log p(\\beta) \\\\\n",
    "S & = -\\nabla^2 \\log p(\\beta|y,x) |_{\\beta_{\\text{MAP}}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_post(beta, x, y, m0, S0):\n",
    "    logprior =  - .5* (beta-m0).T @ S0 @ (beta-m0)\n",
    "    return  nll(beta, x, y) -  logprior\n",
    "def neg_post_prime(beta, x, y, m0, S0):\n",
    "    return  x.T.dot(sigma(x,beta)-y)+S0.dot(beta-m0)\n",
    "def laplace_prec(beta, x, S0):\n",
    "    n = x.shape[0]  \n",
    "    S = np.diag(sigma(x,beta)*(1-sigma(x,beta)))\n",
    "    prec = S0.values + x.T.dot(S.dot(x))    \n",
    "    return prec\n",
    "def NewtonRaphsonBayesLogReg(beta0,x,y,m0,S0,maxiter,tolerance):\n",
    "    i = 0\n",
    "    beta = beta0\n",
    "    NegLogPost = neg_post(beta, x, y, m0, S0)\n",
    "    AbDiff = 1; #anything bigger than tolerance would do here\n",
    "    while (AbDiff> tolerance) & (i<maxiter):\n",
    "        i =i+1;\n",
    "        print('iteration ',i,' Negative Log Posterior ',NegLogPost, ' AbDiff ', AbDiff)\n",
    "        grad = neg_post_prime(beta, x, y, m0, S0)\n",
    "        H = laplace_prec(beta, x, S0)\n",
    "        H_inv = sc.linalg.inv(H) \n",
    "        beta = beta - H_inv.dot(grad)\n",
    "        NegLogPost_new = neg_post(beta, x, y, m0, S0)\n",
    "        AbDiff = np.abs(NegLogPost_new-NegLogPost)\n",
    "        NegLogPost = NegLogPost_new\n",
    "    if (iter == maxiter):\n",
    "        print('Did not Converge') \n",
    "    return beta, H_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = X_train.shape\n",
    "m0 = np.zeros(p)\n",
    "S0 = X_train.T.dot(X_train)/n\n",
    "beta0 = np.zeros(p)\n",
    "tolerance = 1e-05\n",
    "maxiter = 100 \n",
    "beta_map, cov = NewtonRaphsonBayesLogReg(beta0,X_train, y_train['y'],\n",
    "                                                  m0,S0,maxiter,tolerance)\n",
    "print(beta_map)\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#95% credible intervals\n",
    "se = np.sqrt(np.diag(cov))\n",
    "lower95 = beta_map - 1.96*se\n",
    "upper95 = beta_map + 1.96*se\n",
    "\n",
    "#present the output via a pandas data frame\n",
    "results = np.column_stack([beta_map,se,lower95,upper95])\n",
    "col = ['post mean','post se','lower 95% bound','upper 95% bound']\n",
    "ind = ['intercept','age','nodes_detected']\n",
    "results = pd.DataFrame(results,columns = col,index=ind)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1\n",
    "\n",
    "Consider the model that also has the variable year as a covariate. Provide the ouput from the MLE and Bayesian (via Laplace approximation) approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Put your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictive performance for Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate model evidence and BIC\n",
    "n,p = X_train.shape\n",
    "logEv = -neg_post(beta_map, X_train, y_train, m0, S0) + (p/2)*np.log(2*np.pi) + (1/2)*np.log(sc.linalg.det(cov))\n",
    "BIC = -nll(beta_map, X_train, y_train) -(p/2)*np.log(n)\n",
    "logEv, BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction assessment for MLE approach\n",
    "predict_probs = sigma(X_test,beta_mle)\n",
    "predict_class = (predict_probs > .5)\n",
    "\n",
    "AccRate_MLE = np.sum(predict_class==y_test['y'])/len(y_test) \n",
    "LS_MLE = -(y_test.T.dot(np.log(predict_probs))+(1-y_test).T.dot(np.log(1-predict_probs)))\n",
    "\n",
    "print(AccRate_MLE, LS_MLE)\n",
    "print(accuracy_score(y_test,predict_class), log_loss(y_test, predict_probs, normalize=False))\n",
    "\n",
    "fpr,tpr,thresholds = roc_curve(y_test,predict_probs)\n",
    "xgrid = np.linspace(0,1,100) \n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(xgrid,xgrid,'red')\n",
    "AUC_MLE = roc_auc_score(y_test,predict_probs)\n",
    "print(AUC_MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the prediction probabilities with Bayesian Logistic regression\n",
    "\n",
    "N = 10000 #number of Monte Carlo samples\n",
    "# draw N samples from the approximate posterior (Laplace) of beta\n",
    "betas = np.random.multivariate_normal(beta_map,cov,N)\n",
    "Xt = X_test.values[0,]\n",
    "print(np.mean(Xt.dot(betas.T)))\n",
    "Xbetas = X_test.dot(betas.T)\n",
    "test = np.mean(Xbetas, axis=1)\n",
    "test.head()\n",
    "\n",
    "#apply sigmoid\n",
    "d = 1/(1. + np.exp(-Xbetas))\n",
    "pred_probs_blr = np.mean(d, axis=1)\n",
    "plt.plot(pred_probs_blr,'.')\n",
    "plt.plot(predict_probs,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate predictive performance of Bayesian Logistic Regression\n",
    "\n",
    "pred_class_blr = (pred_probs_blr > .5)\n",
    "\n",
    "AccRate = np.sum(pred_class_blr==y_test['y'])/len(y_test) \n",
    "LS = -(y_test.T.dot(np.log(pred_probs_blr))+(1-y_test).T.dot(np.log(1-pred_probs_blr)))\n",
    "\n",
    "fpr,tpr,thresholds = roc_curve(y_test,pred_probs_blr)\n",
    "xgrid = np.linspace(0,1,100) \n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(xgrid,xgrid,'red')\n",
    "AUC = roc_auc_score(y_test,pred_probs_blr)\n",
    "print(AccRate, LS, AUC)\n",
    "print(AccRate_MLE, LS_MLE, AUC_MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "Consider 2 logistic regression models: \n",
    "\n",
    " - M1: with covariates *age* and *nodes detected*.\n",
    " - M2: with covariates *year*, *age* and *nodes detected*.\n",
    " \n",
    " Compare them using the log-evidence, the BIC, the area under the ROC curve and the log scoring rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Put your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "\n",
    "Compute the classification probability $p(y=1|x)$ by modeling the joint probability $p(y,x)$. That's called a generative model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data and create train test splits\n",
    "X = data1.loc[:,['age', 'nodes_detected']].values\n",
    "y = data1.survival_status.replace(2, 0).values.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                        test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first present a solution using `sklearn`. The following simple commands suffice to train a Linear Discriminant Analysis classifier using the `sklearn` (also known as sci-kit package).\n",
    "\n",
    "In our setting we have a binary (2-class) classification problem with a sample size $n$. Our date consistts of pairs of labels and features $\\{(y_1,x_1), \\ldots, (y_n,x_n)\\}$ where $y_i \\in \\{0,1\\}$ and $x_i \\in R^m$. We are interested in the probability $p(y=1|x)$.\n",
    "\n",
    "The formulas used are the following equations in Bishop\n",
    "\n",
    "* Prior probability of class 0 : 4.73\n",
    "* Mean of features 4.75, 4.76\n",
    "* Covariance matrix 4.78\n",
    "\n",
    "which are implemented by the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='lsqr',store_covariance=True)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.means_, clf.priors_, clf.covariance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also predict the class of new individuals based on their $X$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_lda = clf.predict_proba(X_test)\n",
    "pred_probs_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially we only need the probability of y=1 (2nd column). Probability of y=0 is just one minus that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_lda = pred_probs_lda[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3\n",
    "\n",
    "Fit logistic regression and LDA models on the training data and evaluate their predictive performance (ROC, area under the ROC, log score) on the test data. Use *year*, *age* and *nodes_detected* as covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Put your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
