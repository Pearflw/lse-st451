---
title: "ComputerClass08_CodeActivities"
author: "Kostas Kalogeropoulos"
date: "31/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages and Data

```{r}
library("rstan")
rstan_options(auto_write = TRUE)
```

Load data from the saved files we worked in Python

```{r}
X_train <- fread('X_train.csv', header = FALSE)
X_test <- fread('X_test.csv', header = FALSE)
y_train <- fread('y_train.csv', header = FALSE)
y_test <- fread('y_test.csv', header = FALSE)

p=dim(X_train)[2]
n=length(as.vector(t(y_train)))
```

Prepare data for Stan

```{r}
diabetes_dat<-list(n=n, p=p, X=X_train, y=as.vector(t(y_train)))
```

## Bayesian linear regression with N(0,1) priors

For demonstration purposes we will run fit the linear regression model on the data with standard N(0,1) priors. We will assign a Cauchy(0,1) on sigma though to deviate from the conjugate model. See 'LinearRegression.stan' for the file with the stan code.

```{r}
NormalLR <-stan(file = 'LinearRegression.stan', data = diabetes_dat,chains=1,init=0,seed=1)
```

```{r}
summary(NormalLR,pars=c('beta','sigma'))
params=c('beta[1]','beta[7]','beta[12]','beta[21]','beta[34]','beta[47]','beta[56]','beta[64]','sigma')
traceplot(NormalLR,pars=params) #some indicative betas; too many to see all
```

This is all good but there are tricks to defining models in stan so that the MCMC mixing is more likely to be goo. In other wordsreparametrisation. See 'LinearRegression2.stan' for a more robust way to define the same model. It probably won't make a difference here but you will need it for the horseshoe later.

```{r}
NormalLR2 <-stan(file = 'LinearRegression2.stan', data = diabetes_dat,chains=1,init=0,seed=1)
```

```{r}
summary(NormalLR2,pars=c('beta','sigma'))
params=c('beta[1]','beta[7]','beta[12]','beta[21]','beta[34]','beta[47]','beta[56]','beta[64]','sigma')
traceplot(NormalLR2,pars=params) #some indicative betas; too many to see all
```

Finaly let's check the predictive ability of this model. We will use the posterior mean as a point estimate for beta's and predict the $y$ values in the test dataset have the covariates 'X_test' as inputs. The code below does that and computes the MSE.

```{r}
Xte = as.matrix(X_test,nrow=100,ncol=64)
draws = extract(NormalLR2,pars=c('beta','sigma'))
beta_n = apply(draws$beta,2,mean)
y_pred = Xte %*% beta_n;
mean((y_test$V1 - y_pred)**2)
```

MSE is similar to Ridge regression which is not surprising.

## Activity 2: Horseshoe prior

Write the code in stan, in the spirit of the 'LinearRegression2.stan', the model with the Horseshoe prior
$$
\beta_i \sim N(0,\lambda_i\tau),\;\;\lambda_i\sim\text{Cauchy}(0,1),\;\;\tau\sim\text{Cauchy}(0,1).
$$
Put your code below and assess the predictive performance as before using the test data.
